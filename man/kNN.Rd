% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kNN.R
\name{kNN}
\alias{kNN}
\title{kNN}
\usage{
kNN(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
}
\arguments{
\item{train}{training dataset with m data samples and d dimensions. A matrix of m by d.}

\item{test}{test dataset with n data samples and d dimensions. A matrix of n by d}

\item{cl}{training ground-true class labels for m data samples. A vector of length m.}

\item{k}{the number of nearest neighbors}

\item{l}{minimum vote to make decision for prediction, i.e. less than k-l dissenting votes are allowed, even if k may be larger than inputted k due to ties.}

\item{prob}{a boolean. If \code{TRUE}, prediction probability is returned as output's attribute. If \code{FALSE}, only prediction labels are returned.}

\item{use.all}{controls the way of handling ties. If \code{TRUE}, all distances equal to the kth largest are used. If \code{FALSE}, a random selection of distances equal to the k-th nearest distance is chosen to select exactly k neighbours.}
}
\value{
the predicted classfication of test
}
\description{
Classfication Algorithm of K Nearest Neighbor (kNN) implemented by Rcpp and accelerated by multi-thread
}
\details{
Input a training dataset and its corresponding class labels, return the predicted classfications by taking majority votes of k-nearest neighbors' classes.
}
\examples{
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
preds = kNN(train, test, cl, k = 10, prob=TRUE, use.all = T)
probs=attributes(preds)$prob

}
